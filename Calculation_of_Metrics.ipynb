{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FV5T2eFnOcVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927a57c6-3e7a-43f6-b1d4-f7411735499e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/NYU/Courses/DSGA1011/Project/output_QA/arc_nos_cleaned/arc_nos_cleaned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEDvVJsEVI0c",
        "outputId": "570a250b-c481-42e4-aeba-d79393cf9bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NYU/Courses/DSGA1011/Project/output_QA/arc_nos_cleaned/arc_nos_cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/NYU/Courses/DSGA1011/Project/output_1011_parameter/output_1011_parameter/truthfuqa_noc_cleaned/truthfuqa_noc_cleaned"
      ],
      "metadata": {
        "id": "SHidZDPDOyCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dbae3f-4be4-4799-cd86-c94cd34c1bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NYU/Courses/DSGA1011/Project/output_1011_parameter/output_1011_parameter/truthfuqa_noc_cleaned/truthfuqa_noc_cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas._config.config import option_context\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YUtpI022Qf_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"truthfulQA_16_GPT_Cleaned.csv\""
      ],
      "metadata": {
        "id": "Kr3_PHk9QEmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(filename, encoding='latin-1')\n",
        "\n",
        "df['GPT inferred confidence level'] = pd.to_numeric(df['GPT inferred confidence level'], errors='coerce')\n",
        "#df['Confidence Level'] = pd.to_numeric(df['Confidence Level'], errors='coerce')\n",
        "df = df.dropna()\n",
        "print(df.shape[0])\n",
        "\n",
        "count_1 = df[(df['GPT inferred answer'] == df['Correct Answer'])].shape[0]\n",
        "count_2 = df[(df['GPT inferred answer'] != df['Correct Answer']) & (df['GPT inferred confidence level'] >= 80)].shape[0]\n",
        "#count_2 = df[(df['GPT inferred answer'] != df['answer']) & (df['Confidence Level'] >= 80)].shape[0]\n",
        "print(count_1 / len(df))\n",
        "print(count_2 / len(df))\n",
        "def calculate_ece(predictions, true_labels, confidences, threshold=80, num_bins=10):\n",
        "    confidences = confidences / 100\n",
        "    threshold = threshold / 100\n",
        "    incorrect = np.where(predictions != true_labels)\n",
        "    i_confidences = confidences[incorrect]\n",
        "\n",
        "    bin_edges = np.linspace(0, 1, num_bins+1)\n",
        "    bin_indices_o = np.digitize(confidences, bin_edges, right=True)\n",
        "    bin_indices_oi = np.digitize(i_confidences, bin_edges, right=True)\n",
        "\n",
        "    ece = 0\n",
        "    total_instances = len(bin_indices_o)\n",
        "\n",
        "    for bin_idx in range(0, num_bins + 1):\n",
        "        instances_in_bin = np.where(bin_indices_o == bin_idx)\n",
        "        if len(instances_in_bin[0]) == 0:\n",
        "            continue\n",
        "        avg_confidence = np.mean(confidences[instances_in_bin])\n",
        "        observed_accuracy = np.count_nonzero(true_labels[instances_in_bin] == predictions[instances_in_bin]) / len(instances_in_bin[0])\n",
        "\n",
        "        ece += (len(instances_in_bin[0]) / total_instances) * np.abs(avg_confidence - observed_accuracy)\n",
        "\n",
        "    return ece\n",
        "\n",
        "df = df[pd.notnull(df['GPT inferred confidence level'])]\n",
        "predictions = df['GPT inferred answer'].values\n",
        "true_labels = df['Correct Answer'].values\n",
        "confidences = df['GPT inferred confidence level'].values\n",
        "#confidences = df['Confidence Level'].values\n",
        "\n",
        "ece = calculate_ece(predictions, true_labels, confidences)\n",
        "print(\"ECE for overconfident predictions:\", ece)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg0fJaLRxJdO",
        "outputId": "ac370452-366e-4c95-b535-fafa02d55efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157\n",
            "0.27388535031847133\n",
            "0.5605095541401274\n",
            "ECE for overconfident predictions: 0.6070063694267516\n"
          ]
        }
      ]
    }
  ]
}