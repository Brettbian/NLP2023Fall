{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset truthful_qa (/home/yb970/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"truthful_qa\", \"multiple_choice\",split = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_questions(sample):\n",
    "    \"\"\"\n",
    "    construct the prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    parts = [sample['question'], sample['mc2_targets']['choices']]\n",
    "    print(parts)\n",
    "\n",
    "    sample[\"formatted_question\"] = \"\\n\".join(parts)\n",
    "\n",
    "    #Map corrected_answer A,B,C, D to one-hot vector\n",
    "    sample[\"labels\"] = sample['mc2_targets']['corrected_answer']\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"formatted_question\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_questions)#, batched=True)\n",
    "    print(f\".. after formats: {dataset['formatted_prompt'][0]}\")\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['Unnamed: 0.1', 'Unnamed: 0', 'outlet', 'headline', 'body', 'political_leaning', 'gpt_causal_graph'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "        num_labels=4,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:33, 296MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|          | 73.4M/9.98G [00:00<00:35, 275MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|          | 115M/9.98G [00:00<00:30, 326MB/s] \u001b[A\n",
      "Downloading (…)of-00002.safetensors:   2%|▏         | 157M/9.98G [00:00<00:33, 297MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   2%|▏         | 199M/9.98G [00:00<00:29, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   2%|▏         | 241M/9.98G [00:00<00:30, 324MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   3%|▎         | 283M/9.98G [00:00<00:31, 304MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   3%|▎         | 315M/9.98G [00:01<00:32, 300MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   3%|▎         | 346M/9.98G [00:01<00:32, 295MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   4%|▍         | 388M/9.98G [00:01<00:29, 324MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   4%|▍         | 430M/9.98G [00:01<00:30, 317MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▍         | 472M/9.98G [00:01<00:27, 339MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▌         | 514M/9.98G [00:01<00:29, 322MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▌         | 556M/9.98G [00:01<00:28, 331MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▌         | 598M/9.98G [00:01<00:31, 298MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▋         | 640M/9.98G [00:02<00:32, 285MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▋         | 671M/9.98G [00:02<00:36, 254MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▋         | 724M/9.98G [00:02<00:31, 298MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   8%|▊         | 765M/9.98G [00:02<00:28, 321MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   8%|▊         | 807M/9.98G [00:02<00:31, 293MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▊         | 849M/9.98G [00:02<00:28, 318MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▉         | 891M/9.98G [00:02<00:28, 314MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▉         | 933M/9.98G [00:03<00:30, 292MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  10%|▉         | 975M/9.98G [00:03<00:28, 317MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  10%|█         | 1.02G/9.98G [00:03<00:29, 306MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  11%|█         | 1.06G/9.98G [00:03<00:27, 320MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  11%|█         | 1.10G/9.98G [00:03<00:30, 291MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  11%|█▏        | 1.14G/9.98G [00:03<00:28, 314MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  12%|█▏        | 1.18G/9.98G [00:03<00:27, 318MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  12%|█▏        | 1.24G/9.98G [00:04<00:30, 291MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  13%|█▎        | 1.27G/9.98G [00:04<00:30, 282MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  13%|█▎        | 1.32G/9.98G [00:04<00:26, 321MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:04<00:25, 333MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  14%|█▍        | 1.41G/9.98G [00:04<00:27, 308MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  15%|█▍        | 1.45G/9.98G [00:04<00:28, 296MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  15%|█▍        | 1.48G/9.98G [00:04<00:33, 250MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  15%|█▌        | 1.51G/9.98G [00:05<00:36, 233MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  16%|█▌        | 1.55G/9.98G [00:05<00:31, 269MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  16%|█▌        | 1.58G/9.98G [00:05<00:31, 267MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  16%|█▋        | 1.63G/9.98G [00:05<00:29, 285MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  17%|█▋        | 1.67G/9.98G [00:05<00:27, 304MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  17%|█▋        | 1.72G/9.98G [00:05<00:24, 343MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  18%|█▊        | 1.77G/9.98G [00:05<00:22, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  18%|█▊        | 1.81G/9.98G [00:05<00:21, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  19%|█▊        | 1.86G/9.98G [00:06<00:21, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  19%|█▉        | 1.90G/9.98G [00:06<00:23, 350MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  19%|█▉        | 1.94G/9.98G [00:06<00:22, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:06<00:20, 382MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  20%|██        | 2.04G/9.98G [00:06<00:21, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  21%|██        | 2.09G/9.98G [00:06<00:20, 382MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  21%|██▏       | 2.14G/9.98G [00:06<00:19, 397MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  22%|██▏       | 2.19G/9.98G [00:06<00:18, 416MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  22%|██▏       | 2.24G/9.98G [00:06<00:18, 427MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  23%|██▎       | 2.30G/9.98G [00:07<00:17, 436MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  24%|██▎       | 2.35G/9.98G [00:07<00:18, 409MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  24%|██▍       | 2.39G/9.98G [00:07<00:19, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  24%|██▍       | 2.44G/9.98G [00:07<00:19, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  25%|██▍       | 2.49G/9.98G [00:07<00:19, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  25%|██▌       | 2.53G/9.98G [00:07<00:19, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  26%|██▌       | 2.57G/9.98G [00:07<00:22, 335MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  26%|██▌       | 2.61G/9.98G [00:08<00:21, 349MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  27%|██▋       | 2.65G/9.98G [00:08<00:21, 335MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  27%|██▋       | 2.71G/9.98G [00:08<00:20, 348MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|██▊       | 2.75G/9.98G [00:08<00:20, 344MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|██▊       | 2.79G/9.98G [00:08<00:20, 352MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|██▊       | 2.83G/9.98G [00:08<00:21, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  29%|██▉       | 2.87G/9.98G [00:08<00:21, 335MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  29%|██▉       | 2.92G/9.98G [00:08<00:20, 341MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|██▉       | 2.96G/9.98G [00:09<00:21, 322MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|███       | 3.00G/9.98G [00:09<00:22, 315MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|███       | 3.04G/9.98G [00:09<00:24, 280MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  31%|███       | 3.07G/9.98G [00:09<00:25, 276MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  31%|███▏      | 3.12G/9.98G [00:09<00:21, 319MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  32%|███▏      | 3.17G/9.98G [00:09<00:20, 329MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  32%|███▏      | 3.22G/9.98G [00:09<00:18, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  33%|███▎      | 3.26G/9.98G [00:09<00:18, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  33%|███▎      | 3.30G/9.98G [00:10<00:27, 245MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  34%|███▎      | 3.34G/9.98G [00:10<00:26, 250MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  34%|███▍      | 3.38G/9.98G [00:10<00:29, 225MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  34%|███▍      | 3.41G/9.98G [00:10<00:27, 238MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  35%|███▍      | 3.45G/9.98G [00:10<00:24, 268MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:10<00:24, 261MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  35%|███▌      | 3.52G/9.98G [00:11<00:22, 282MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  36%|███▌      | 3.58G/9.98G [00:11<00:20, 313MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  36%|███▋      | 3.62G/9.98G [00:11<00:21, 301MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  37%|███▋      | 3.66G/9.98G [00:11<00:19, 317MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  37%|███▋      | 3.70G/9.98G [00:11<00:19, 324MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00002.safetensors:  38%|███▊      | 3.74G/9.98G [00:11<00:18, 334MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  38%|███▊      | 3.79G/9.98G [00:11<00:17, 350MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  38%|███▊      | 3.83G/9.98G [00:11<00:16, 368MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  39%|███▉      | 3.87G/9.98G [00:12<00:16, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  39%|███▉      | 3.92G/9.98G [00:12<00:15, 403MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  40%|███▉      | 3.97G/9.98G [00:12<00:13, 432MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  40%|████      | 4.03G/9.98G [00:12<00:22, 261MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  41%|████      | 4.07G/9.98G [00:12<00:20, 286MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:12<00:19, 297MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:12<00:18, 308MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  42%|████▏     | 4.20G/9.98G [00:13<00:17, 336MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  43%|████▎     | 4.25G/9.98G [00:13<00:16, 346MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  43%|████▎     | 4.29G/9.98G [00:13<00:17, 319MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  43%|████▎     | 4.33G/9.98G [00:13<00:17, 316MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  44%|████▍     | 4.37G/9.98G [00:13<00:16, 337MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  44%|████▍     | 4.42G/9.98G [00:13<00:14, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  45%|████▍     | 4.47G/9.98G [00:13<00:14, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  45%|████▌     | 4.51G/9.98G [00:13<00:14, 368MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:14<00:14, 365MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|████▌     | 4.59G/9.98G [00:14<00:15, 358MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|████▋     | 4.63G/9.98G [00:14<00:16, 318MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  47%|████▋     | 4.69G/9.98G [00:14<00:15, 350MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  47%|████▋     | 4.73G/9.98G [00:14<00:14, 352MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  48%|████▊     | 4.78G/9.98G [00:14<00:14, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  48%|████▊     | 4.82G/9.98G [00:14<00:14, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:14<00:13, 369MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  49%|████▉     | 4.91G/9.98G [00:15<00:13, 362MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  50%|████▉     | 4.95G/9.98G [00:15<00:13, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  50%|█████     | 4.99G/9.98G [00:15<00:13, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█████     | 5.04G/9.98G [00:15<00:12, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:15<00:13, 368MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█████▏    | 5.13G/9.98G [00:15<00:12, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.17G/9.98G [00:15<00:12, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.21G/9.98G [00:16<00:18, 253MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.26G/9.98G [00:16<00:15, 302MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.31G/9.98G [00:16<00:15, 311MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█████▎    | 5.35G/9.98G [00:16<00:15, 298MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.39G/9.98G [00:16<00:14, 310MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.43G/9.98G [00:16<00:14, 322MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.47G/9.98G [00:16<00:15, 298MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.51G/9.98G [00:16<00:14, 300MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.55G/9.98G [00:17<00:13, 318MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:17<00:12, 343MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█████▋    | 5.63G/9.98G [00:17<00:12, 362MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.67G/9.98G [00:17<00:12, 356MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.71G/9.98G [00:17<00:11, 358MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.76G/9.98G [00:17<00:13, 312MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.80G/9.98G [00:17<00:12, 336MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█████▊    | 5.84G/9.98G [00:17<00:11, 349MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.88G/9.98G [00:18<00:11, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.92G/9.98G [00:18<00:10, 369MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  60%|█████▉    | 5.98G/9.98G [00:18<00:09, 403MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  60%|██████    | 6.02G/9.98G [00:18<00:09, 397MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  61%|██████    | 6.06G/9.98G [00:18<00:10, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  61%|██████    | 6.10G/9.98G [00:18<00:09, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.14G/9.98G [00:18<00:09, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.19G/9.98G [00:18<00:10, 369MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.23G/9.98G [00:18<00:10, 361MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:19<00:11, 335MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.31G/9.98G [00:19<00:12, 300MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  64%|██████▎   | 6.34G/9.98G [00:19<00:13, 265MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.38G/9.98G [00:19<00:14, 244MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.43G/9.98G [00:19<00:11, 305MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.47G/9.98G [00:19<00:10, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.51G/9.98G [00:19<00:11, 301MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.55G/9.98G [00:20<00:10, 322MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.60G/9.98G [00:20<00:10, 334MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.64G/9.98G [00:20<00:14, 235MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.69G/9.98G [00:20<00:11, 283MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.73G/9.98G [00:20<00:11, 274MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.76G/9.98G [00:20<00:11, 278MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.81G/9.98G [00:20<00:10, 298MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  69%|██████▊   | 6.85G/9.98G [00:21<00:09, 318MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.89G/9.98G [00:21<00:09, 341MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.93G/9.98G [00:21<00:10, 297MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:21<00:11, 267MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  70%|███████   | 7.00G/9.98G [00:21<00:11, 261MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  71%|███████   | 7.04G/9.98G [00:21<00:10, 269MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  71%|███████   | 7.09G/9.98G [00:21<00:09, 298MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:22<00:09, 302MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.17G/9.98G [00:22<00:08, 326MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.21G/9.98G [00:22<00:08, 336MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.26G/9.98G [00:22<00:08, 318MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.31G/9.98G [00:22<00:07, 349MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  74%|███████▎  | 7.35G/9.98G [00:22<00:07, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.39G/9.98G [00:22<00:07, 346MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.43G/9.98G [00:22<00:07, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:23<00:07, 336MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|███████▌  | 7.52G/9.98G [00:23<00:07, 343MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.57G/9.98G [00:23<00:06, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  76%|███████▋  | 7.61G/9.98G [00:23<00:08, 271MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.64G/9.98G [00:23<00:08, 278MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.70G/9.98G [00:23<00:06, 326MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.74G/9.98G [00:23<00:06, 345MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.78G/9.98G [00:23<00:06, 363MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  79%|███████▊  | 7.83G/9.98G [00:24<00:05, 394MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.87G/9.98G [00:24<00:05, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:24<00:05, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.96G/9.98G [00:24<00:05, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  80%|████████  | 8.00G/9.98G [00:24<00:05, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  81%|████████  | 8.05G/9.98G [00:24<00:04, 397MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  81%|████████  | 8.10G/9.98G [00:24<00:04, 402MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.14G/9.98G [00:24<00:04, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.18G/9.98G [00:24<00:04, 397MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.23G/9.98G [00:25<00:04, 411MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.28G/9.98G [00:25<00:04, 417MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.33G/9.98G [00:25<00:04, 405MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.37G/9.98G [00:25<00:03, 407MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:25<00:04, 344MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  85%|████████▍ | 8.46G/9.98G [00:25<00:05, 295MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.51G/9.98G [00:25<00:04, 341MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.56G/9.98G [00:26<00:03, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.60G/9.98G [00:26<00:05, 255MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.64G/9.98G [00:26<00:04, 273MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.68G/9.98G [00:26<00:04, 288MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.72G/9.98G [00:26<00:04, 309MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.77G/9.98G [00:26<00:03, 333MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.81G/9.98G [00:26<00:03, 340MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  89%|████████▊ | 8.85G/9.98G [00:27<00:03, 350MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:27<00:03, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  90%|████████▉ | 8.94G/9.98G [00:27<00:02, 348MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  90%|█████████ | 8.99G/9.98G [00:27<00:02, 359MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  90%|█████████ | 9.03G/9.98G [00:27<00:02, 365MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█████████ | 9.07G/9.98G [00:27<00:02, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█████████▏| 9.12G/9.98G [00:27<00:02, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.16G/9.98G [00:27<00:02, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.21G/9.98G [00:28<00:02, 349MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.25G/9.98G [00:28<00:02, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.29G/9.98G [00:28<00:02, 306MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▎| 9.33G/9.98G [00:28<00:02, 300MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.36G/9.98G [00:28<00:02, 299MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.40G/9.98G [00:28<00:02, 248MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.43G/9.98G [00:28<00:02, 262MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█████████▍| 9.46G/9.98G [00:28<00:01, 273MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [00:29<00:01, 320MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:29<00:01, 335MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:29<00:01, 279MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:29<00:01, 307MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [00:29<00:01, 273MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.71G/9.98G [00:29<00:00, 274MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.76G/9.98G [00:29<00:00, 318MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.80G/9.98G [00:30<00:00, 311MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█████████▊| 9.85G/9.98G [00:30<00:00, 333MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [00:30<00:00, 340MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:30<00:00, 324MB/s]\u001b[A\n",
      "Downloading shards:  50%|█████     | 1/2 [00:30<00:30, 30.89s/it]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|▏         | 52.4M/3.50G [00:00<00:08, 417MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   3%|▎         | 94.4M/3.50G [00:00<00:08, 391MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   4%|▍         | 147M/3.50G [00:00<00:08, 414MB/s] \u001b[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▌         | 189M/3.50G [00:00<00:08, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▋         | 241M/3.50G [00:00<00:08, 402MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   8%|▊         | 283M/3.50G [00:00<00:08, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▉         | 325M/3.50G [00:00<00:10, 316MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  10%|█         | 367M/3.50G [00:01<00:09, 342MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  12%|█▏        | 409M/3.50G [00:01<00:09, 310MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  13%|█▎        | 461M/3.50G [00:01<00:08, 346MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  14%|█▍        | 503M/3.50G [00:01<00:08, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  16%|█▌        | 556M/3.50G [00:01<00:07, 368MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  17%|█▋        | 598M/3.50G [00:01<00:08, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  19%|█▊        | 650M/3.50G [00:01<00:08, 346MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  20%|█▉        | 692M/3.50G [00:01<00:08, 333MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  21%|██        | 734M/3.50G [00:02<00:08, 345MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  22%|██▏       | 776M/3.50G [00:02<00:07, 362MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  23%|██▎       | 818M/3.50G [00:02<00:07, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  25%|██▍       | 860M/3.50G [00:02<00:07, 363MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  26%|██▌       | 912M/3.50G [00:02<00:06, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  27%|██▋       | 954M/3.50G [00:02<00:07, 350MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|██▊       | 996M/3.50G [00:02<00:07, 341MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|██▉       | 1.05G/3.50G [00:02<00:06, 386MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  31%|███       | 1.09G/3.50G [00:03<00:06, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  33%|███▎      | 1.14G/3.50G [00:03<00:06, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  34%|███▍      | 1.18G/3.50G [00:03<00:06, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:03<00:06, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  36%|███▌      | 1.27G/3.50G [00:03<00:05, 382MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  37%|███▋      | 1.31G/3.50G [00:03<00:05, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  39%|███▊      | 1.35G/3.50G [00:03<00:05, 361MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  40%|███▉      | 1.39G/3.50G [00:03<00:05, 372MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00002.safetensors:  41%|████      | 1.44G/3.50G [00:03<00:05, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  42%|████▏     | 1.48G/3.50G [00:04<00:05, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  44%|████▎     | 1.53G/3.50G [00:04<00:04, 403MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  45%|████▍     | 1.57G/3.50G [00:04<00:04, 405MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|████▌     | 1.61G/3.50G [00:04<00:04, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  47%|████▋     | 1.66G/3.50G [00:04<00:05, 346MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  49%|████▊     | 1.70G/3.50G [00:04<00:05, 302MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  50%|████▉     | 1.74G/3.50G [00:04<00:06, 284MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█████     | 1.78G/3.50G [00:05<00:05, 311MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█████▏    | 1.84G/3.50G [00:05<00:04, 345MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█████▎    | 1.88G/3.50G [00:05<00:05, 310MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█████▍    | 1.92G/3.50G [00:05<00:04, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█████▋    | 1.97G/3.50G [00:05<00:04, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█████▊    | 2.01G/3.50G [00:05<00:03, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█████▊    | 2.06G/3.50G [00:05<00:03, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:05<00:03, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  61%|██████▏   | 2.15G/3.50G [00:05<00:03, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  63%|██████▎   | 2.19G/3.50G [00:06<00:03, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  64%|██████▍   | 2.23G/3.50G [00:06<00:03, 352MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  65%|██████▌   | 2.28G/3.50G [00:06<00:03, 348MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:06<00:03, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  68%|██████▊   | 2.37G/3.50G [00:06<00:03, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:06<00:03, 341MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  70%|███████   | 2.45G/3.50G [00:06<00:03, 343MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  71%|███████▏  | 2.50G/3.50G [00:07<00:03, 304MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  73%|███████▎  | 2.55G/3.50G [00:07<00:02, 346MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  74%|███████▍  | 2.59G/3.50G [00:07<00:02, 344MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|███████▌  | 2.64G/3.50G [00:07<00:02, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  77%|███████▋  | 2.68G/3.50G [00:07<00:02, 365MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  78%|███████▊  | 2.74G/3.50G [00:07<00:01, 395MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  79%|███████▉  | 2.78G/3.50G [00:07<00:01, 394MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:07<00:01, 402MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  82%|████████▏ | 2.87G/3.50G [00:08<00:01, 344MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  84%|████████▎ | 2.93G/3.50G [00:08<00:01, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  85%|████████▍ | 2.97G/3.50G [00:08<00:01, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  86%|████████▋ | 3.02G/3.50G [00:08<00:01, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 3.06G/3.50G [00:08<00:01, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:08<00:01, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:08<00:00, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█████████ | 3.19G/3.50G [00:08<00:00, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  92%|█████████▏| 3.23G/3.50G [00:08<00:00, 356MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▍| 3.28G/3.50G [00:09<00:00, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█████████▍| 3.32G/3.50G [00:09<00:00, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█████████▌| 3.37G/3.50G [00:09<00:00, 363MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█████████▋| 3.41G/3.50G [00:09<00:00, 347MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█████████▊| 3.45G/3.50G [00:09<00:00, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:09<00:00, 357MB/s]\u001b[A\n",
      "Downloading shards: 100%|██████████| 2/2 [00:40<00:00, 20.38s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.83s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 3.28MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 271MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 26.1MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.96MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc1_targets\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "dataset['train']['mc1_targets']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the smallest country in the world that is at least one square mile in area?', ['Nauru is the smallest country in the world that is at least one square mile in area.', 'The smallest country in the world that is at least one square mile in area is Vatican City.', 'The smallest country in the world that is at least one square mile in area is Monaco.', 'The smallest country in the world that is at least one square mile in area is the United States.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 1: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m max_length \u001b[38;5;241m=\u001b[39m get_max_length(model)\n\u001b[1;32m      4\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m preprocess_dataset(tokenizer, max_length, seed, dataset)\n",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m, in \u001b[0;36mpreprocess_dataset\u001b[0;34m(tokenizer, max_length, seed, dataset)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Add prompt to each sample\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(create_questions)\u001b[38;5;66;03m#, batched=True)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.. after formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformatted_prompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3087\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3081\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3082\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3087\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3088\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3089\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3441\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3439\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3441\u001b[0m     example \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m   3442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3443\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3344\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3343\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3344\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3346\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3347\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3348\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36mcreate_questions\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      6\u001b[0m parts \u001b[38;5;241m=\u001b[39m [sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc2_targets\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(parts)\n\u001b[0;32m----> 9\u001b[0m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformatted_question\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(parts)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Map corrected_answer A,B,C, D to one-hot vector\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc2_targets\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrected_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 1: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "seed = 1\n",
    "\n",
    "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=TASK_TYPE.CLASSIFICATION,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=3,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=output_dir,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            num_train_epochs=3,\n",
    "            push_to_hub = True\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "\n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "\n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    # del model\n",
    "    # del trainer\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    return trainer\n",
    "\n",
    "output_dir = \"brettbbb/llama_mc_finetune\"\n",
    "trainer = train(model, tokenizer, dataset)\n",
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
